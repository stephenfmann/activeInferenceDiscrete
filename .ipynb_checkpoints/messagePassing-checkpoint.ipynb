{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message passing for variational inference in causal graphs\n",
    "\n",
    "Say you have a causal graph, and you want to perform variational Bayesian inference on it. You can do this the usual way, minimizing the free energy directly with a lot of maths tailored specifically for your model + maybe gradient descent. Or you can use _message passing_. So I am writing this post in an attempt to learn message passing.\n",
    "\n",
    "The material discussed in this post all comes from [this](http://www.jmlr.org/papers/volume6/winn05a/winn05a.pdf). Some general remarks: \n",
    "- VPM applies when the nodes correspond to conditional distributions in the exponential family.\n",
    "- In VMP, the messages are exponential family distributions \n",
    "- summarised either by their natural parameter vector (for child-to-parent messages) or by a vector of moments (for parent-to-child messages). \n",
    "- These messages are defined so that the optimal variational distribution for a node can be found by summing the messages from its children together with a function of the messages from its parents, where this function depends on the conditional distribution for the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important equation is:\n",
    "\n",
    "$$\n",
    "\\ln(P(\\textbf{V})) = \\mathcal{L}(Q) + KL( Q || P )\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{V}$ is the set of observable variables in the graph\n",
    "- $\\mathbf{H}$ is the set of hidden variables.\n",
    "- $P( \\mathbf{H} | \\mathbf{V} )$ is the true posterior distribution, which we don't know.\n",
    "- $P( \\mathbf{V} )$ is the marginal likelihood, which we cannot calculate directly.\n",
    "- $Q( \\mathbf{H} )$ is the approximate distribution which we are trying to make as close as possible to the posterior $P( \\mathbf{H} | \\mathbf{V} )$.\n",
    "- $KL( Q || P ) $ is the Kullbach-Leibler divergence from $Q$ to $P$.\n",
    "\n",
    "So the main thing about this equation is that the $KL$ is always non-negative and $\\ln \\left(P(\\textbf{V})\\right)$ is fixed. Therefore, to minimize the $KL$, we should maximise the lower-bound $\\mathcal{L}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(Q) = \\sum_\\mathbf{H} Q(\\mathbf{H}) \\ln \\left( \\frac{P( \\mathbf{H}, \\mathbf{V} )}{Q(\\mathbf{H})} \\right)\n",
    "$$\n",
    "\n",
    "or, equivalently, minimize the free energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is all to be helpful, $Q$ has to be simpler than $P$. One way to simplify $P$ is to assume that disjoint groups of variables are independent, so that we write $Q$ as:\n",
    "\n",
    "$$\n",
    "\\prod_i Q_i(\\mathbf{H}_i)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{H}_i$ is a group of variables for each $i$. This corresponds to the assumptions that the different groups of variables are mutually independent - our estimation of the value of a variable in one of the groups does not depend on the value of any of the variables in any of the other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under this assumption, the negative free energy $\\mathcal{L}$ takes a more specific form:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(Q) \n",
    "& = \\sum_\\mathbf{H} \\prod_i \\left( Q_i(\\mathbf{H}_i) \\right) \\ln \\left( \\frac{P( \\mathbf{H}, \\mathbf{V} )}{\\prod_i \\left( Q_i(\\mathbf{H}_i) \\right)} \\right) \\\\\n",
    "& = \n",
    "\\sum_\\mathbf{H} \\left( \\prod_i \\left( Q_i(\\mathbf{H}_i) \\right) \\ln \\left( P( \\mathbf{H}, \\mathbf{V} ) \\right) \\right) - \n",
    "\\sum_\\mathbf{H} \\left( \\prod_i \\left( Q_i(\\mathbf{H}_i) \\right) \\ln \\left( \\prod_i \\left( Q_i(\\mathbf{H}_i) \\right) \\right) \\right) \\\\\n",
    "& = \n",
    "\\sum_\\mathbf{H} \\left( \\prod_i \\left( Q_i(\\mathbf{H}_i) \\right) \\ln \\left( P( \\mathbf{H}, \\mathbf{V} ) \\right) \\right) - \n",
    "\\sum_i \\sum_\\mathbf{H} \\left( \\prod_k \\left( Q_k(\\mathbf{H}_k) \\right) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, just consider the left summand. Take one factor $j$ out of $\\mathbf{H}$ and separate out all the terms in $j$ in the left part of the equation. We can do this because the possible values of $\\mathbf{H}$ is the cartesian product of the values of the variables in $\\mathbf{H}$, and so we can express it as nexted sums across its component dimensions:\n",
    "\n",
    "$$\n",
    "\\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\sum_\\mathbf{H_{-j}} \\left( \\prod_{-j} \\left( Q_{-j}(\\mathbf{H}_{-j}) \\right) \\ln \\left( P( \\mathbf{H}= \\{ \\mathbf{H_j}, \\mathbf{H_{-j}} \\}, \\mathbf{V} ) \\right) \\right) - \n",
    "\\sum_i \\sum_\\mathbf{H} \\left( \\prod_k \\left( Q_k(\\mathbf{H}_k) \\right) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal part of the left summand can then be expressed as an expectation of $\\mathbf{H}_{-j}$:\n",
    "\n",
    "$$\n",
    "\\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\mathbb{E}_{\\mathbf{H}_{-j}} \\left[\n",
    "\\ln \\left( P( \\mathbf{H} = \\{ \\mathbf{H_j}, \\mathbf{H_{-j}} \\}, \\mathbf{V} ) \\right) \\right] - \n",
    "\\sum_i \\sum_\\mathbf{H} \\left( \\prod_k \\left( Q_k(\\mathbf{H}_k) \\right) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we log and exponentiate an internal expression, and the above becomes:\n",
    "\n",
    "$$\n",
    "\\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{\\mathbf{Z}}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[\\ln \\left( P( \\mathbf{H} = \\{ \\mathbf{H_j}, \\mathbf{H_{-j}} \\}, \\mathbf{V} ) \\right) \\right] } \\right) - \n",
    "\\sum_i \\sum_\\mathbf{H} \\left( \\prod_k \\left( Q_k(\\mathbf{H}_k) \\right) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) \\right)\n",
    "$$\n",
    "\n",
    "and finally, multiply and divide by $\\mathbf{Z}$ picking a $\\left( \\mathbf{Z} \\right)$ that normalizes $e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ \\ln \\left( P( H= \\{ \\mathbf{H_j}, \\mathbf{H_{-j}} \\}, \\mathbf{V} ) \\right) \\right] }$, and move the nominator out:\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\mathbf{Z} \\right) + \\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ \\ln \\left( P( \\mathbf{H} = \\{ \\mathbf{H_j}, \\mathbf{H_{-j}} \\}, \\mathbf{V} ) \\right) \\right] } \\right) - \n",
    "\\sum_i \\sum_\\mathbf{H} \\left( \\prod_k \\left( Q_k(\\mathbf{H}_k) \\right) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) \\right)\n",
    "$$\n",
    "\n",
    "Call $E(\\mathbf{H}, \\mathbf{V}) = \\ln \\left( P(  \\mathbf{H} = \\{ \\mathbf{H_j}, \\mathbf{H_{-j}} \\}, \\mathbf{V} ) \\right)$ and rewrite as:\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\mathbf{Z} \\right) + \\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] } \\right) - \n",
    "\\sum_i \\sum_\\mathbf{H} \\left( \\prod_k \\left( Q_k(\\mathbf{H}_k) \\right) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the right part, note that the following can be rewritten:\n",
    "\n",
    "$$\n",
    "\\prod_k \\left( Q_k(\\mathbf{H}_k) \\right) = Q_i(\\mathbf{H}_i) Q_{-i}(\\mathbf{H}_{-i})\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "Q_i(\\mathbf{H}_{-i}) = \\prod_{k \\neq i} \\left( Q_k(\\mathbf{H}_k) \\right)\n",
    "$$\n",
    "\n",
    "So we can rewrite the above as follows:\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\mathbf{Z} \\right) + \\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] } \\right) - \n",
    "\\sum_i \\sum_\\mathbf{H} Q_i(\\mathbf{H}_i) Q_{-i}(\\mathbf{H}_{-i}) \\ln \\left( Q_i(\\mathbf{H}_i) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can decompose $\\mathbf{H}$ as we have done above already, getting:\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\mathbf{Z} \\right) + \\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] } \\right) - \n",
    "\\sum_i  \\sum_\\mathbf{H_i} \\sum_\\mathbf{H_{-i}} Q_{-i}(\\mathbf{H}_{-i}) Q_i(\\mathbf{H}_i) \\ln \\left( Q_i(\\mathbf{H}_i) \\right)\n",
    "$$\n",
    "\n",
    "So the internal part of the right summand becomes an expectation under $Q_{-i}(\\mathbf{H}_{-i})$\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\mathbf{Z} \\right) + \\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] } \\right) - \n",
    "\\sum_i  \\sum_\\mathbf{H_i} \\mathbb{E}_{Q_{-i}(\\mathbf{H}_{-i})} \\left[ Q_i(\\mathbf{H}_i) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) \\right] = \\\\\n",
    "\\ln\\left( \\mathbf{Z} \\right) + \\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] } \\right) - \n",
    "\\sum_i  \\sum_\\mathbf{H_i} Q_i(\\mathbf{H}_i) \\ln \\left( Q_i(\\mathbf{H}_i) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the sum of $i$s is the sum over the groups of variables. So we can separate out the $j$th one on the right summand, getting:\n",
    "\n",
    "$$\n",
    "\\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] } \\right) - \n",
    "\\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( Q_j(\\mathbf{H}_j) \\right) - \n",
    "\\sum_{i \\neq j}  \\sum_\\mathbf{H_i} Q_i(\\mathbf{H}_i) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) + \\ln\\left( \\mathbf{Z} \\right)\n",
    "$$\n",
    "\n",
    "We can bring together the two expectations under $j$:\n",
    "\n",
    "$$\n",
    "\\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{\\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] }}{Q_j(\\mathbf{H}_j) } \\right) - \n",
    "\\sum_{i \\neq j}  \\sum_\\mathbf{H_i} Q_i(\\mathbf{H}_i) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) + \\ln\\left( \\mathbf{Z} \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the choice of $\\mathbf{Z}$ makes sense, because that makes the fraction on the left hand side a fraction of two distributions, and therefore the whole expression a negative KL divergence!:\n",
    "\n",
    "$$\n",
    "- \\sum_\\mathbf{H_j} Q_j(\\mathbf{H}_j) \\ln \\left( \\frac{Q_j(\\mathbf{H}_j) }{\\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] }} \\right) - \n",
    "\\sum_{i \\neq j}  \\sum_\\mathbf{H_i} Q_i(\\mathbf{H}_i) \\ln \\left( Q_i(\\mathbf{H}_i) \\right) + \\ln\\left( \\mathbf{Z} \\right) = \\\\\n",
    "- KL\\left[ Q_j || Q_{-j}^* \\right] + \\text{ stuff that is independent of } Q_j\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "Q_{-j}^*(\\mathbf{H}_{-j}) = \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this was really useful, because the aim was to mazimise the expression, which represented the lower bound $\\mathcal{L}$. And we can maximise the expression with respect to node $j$ by minimizing $KL\\left[ Q_j || Q_{-j}^* \\right]$. Now, the KL divergence is minimized (and in fact equal to 0) when the two involved distributions are identical, so all we need to do is solve the following expression where we assume that the two are identical:\n",
    "\n",
    "$$\n",
    "Q_j(\\mathbf{H}_{j}) = Q_{-j}^*(\\mathbf{H}_{-j}) = \\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of message passing is to initialize the distribution for all nodes, and then loop through them repeatedly, updating them according to the equation above until we get some kind of convergence and the equality becomes true for all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how the equation above is used reduce the overall maximization of the lower bound to local operations on single nodes, the concept of _Markov blankets_ will be needed. I'm not gonna explain the intuition here, I'll just assume it. I'll just remind you that in a causal graph (which a directed graph), the Markov blanket for a node $X$ consists of the parents of $X$, $pa(X)$, the children of $X$, $ch(X)$, and all the other parents of the children of $X$ (the co-parents of $X$), $copa(X)$.\n",
    "\n",
    "This is important because it allows us to simplify $Q_{-j}^*$, and more specifically the expectation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the value of $Q_j$ which we are interested in can only depend on its Markov blanket, we can rewrite the following:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] = \\\\\n",
    "\\mathbb{E}_{ mb(j) } \\left[ E(\\mathbf{H} = \\left\\{ \\mathbf{H}_j, mb \\left( j \\right) \\right\\}, \\mathbf{V}) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the definition of $E$ above, we can rewrite this as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P( H= \\{ \\mathbf{H_j}, mb(j) \\}, \\mathbf{V} ) \\right) \\right] = \\\\\n",
    "\\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P(pa(j)) \\, P( H_j | pa(j) ) \\, P(copa(x)) \\, P(ch(j) | copa(j), H_j) \\right) \\right] = \\\\\n",
    "\\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P(pa(j)) \\right) + \\ln \\left( P( H_j | pa(j) ) \\right) + \\ln \\left( P(copa(x)) \\right) + \\ln \\left( P(ch(j) | copa(j), H_j) \\right) \\right] = \\\\\n",
    "\\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P(pa(j)) \\right) \\right] + \\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P( H_j | pa(j) ) \\right) \\right] + \\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P(copa(x)) \\right) \\right] + \\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P(ch(j) | copa(j), H_j) \\right) \\right] = \n",
    "$$\n",
    "\n",
    "If you're wondering what happened to $\\mathbf{V}$, this form implicitly accounts for it because it will be included among the children of some of the nodes. Moreover, we can ignore all terms that do not depend on $H_j$, because they are constant wrt the expectation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P( H_j | pa(j) ) \\right) \\right] + \\mathbb{E}_{ mb(j) } \\left[ \\ln \\left( P(ch(j) | copa(j), H_j) \\right) \\right] + \\text{constant} = \\\\\n",
    "\\mathbb{E}_{ Q(pa(j)) } \\left[ \\ln \\left( P( H_j | pa(j) ) \\right) \\right] + \\mathbb{E}_{ Q(copa(j), ch(j)) } \\left[ \\ln \\left( P(ch(j) | copa(j), H_j) \\right) \\right] + \\text{constant} = \\\\\n",
    "\\mathbb{E}_{ Q(pa(j)) } \\left[ \\ln \\left( P( H_j | pa(j) ) \\right) \\right] + \\mathbb{E}_{ Q(copa(j), ch(j)) } \\left[ \\ln \\left( \\prod_{i \\in ch(j)} P( i | pa(i)) \\right) \\right] + \\text{constant} = \\\\\n",
    "\\mathbb{E}_{ Q(pa(j)) } \\left[ \\ln \\left( P( H_j | pa(j) ) \\right) \\right] + \\mathbb{E}_{ Q(copa(j), ch(j)) } \\left[ \\sum_{i \\in ch(j)} \\ln \\left( P( i | pa(i)) \\right) \\right] + \\text{constant} = \\\\\n",
    "\\mathbb{E}_{ Q(pa(j)) } \\left[ \\ln \\left( P( H_j | pa(j) ) \\right) \\right] + \\sum_{i \\in ch(j)} \\mathbb{E}_{ Q(copa(j), ch(j)) } \\left[ \\ln \\left( P( i | pa(i)) \\right) \\right] + \\text{constant}\n",
    "$$\n",
    "\n",
    "This is great, because now to update the distribution of node $j$ we can use independent messages from $j$'s parents and $j$'s children. Finding out exactly what messages to send becomes _a lot_ easier if we assume that all of the distributions are members of the exponential family of distributions, and each parent is a conjugate of its children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential family of distributions\n",
    "\n",
    "Let's take stock of the situation. We started with the idea of finding a posterior distribution. We have seen that we can find an approximation to it by maximizing a lower bound $\\mathcal{L}$. By making the mean-field approximation, we can find the distribution for node $j$ that maximizises $\\mathcal{L}$ by calculating $\\frac{1}{\\mathbf{Z}}e^{\\mathbb{E}_{\\mathbf{H}_{-j}} \\left[ E(\\mathbf{H}, \\mathbf{V}) \\right] }$. Then, it turned out that to calculate it $j$ can receive independent messages from its parents and from its children. Next, we are going to see that by assuming that the nodes in $P$ have distributions in the exponential family, things get a lot easier.\n",
    "\n",
    "So this is a way of writing members of the exponential family of distributions (in so-called _canonical form_, where $\\mathbf{\\theta}$ is a vector of parameters):\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X} | \\mathbf{\\theta} ) = e^{\\mathbf{\\theta} \\cdot \\mathbf{u} (\\mathbf{X}) + f(\\mathbf{X}) + g(\\mathbf{\\theta})}\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\mathbf{\\theta}$: the _natural parameter vector_ of the distribution. It encodes the information about the parameters.\n",
    "- $\\mathbf{u} (\\mathbf{X})$ is the _natural statistic_ vector. It encodes a way of \"synthesizing\" data that expresses what the distribution needs to know about the data to determine its probability. For instance the probability of a certain vector might only depend on its mean.\n",
    "- $g(\\mathbf{\\theta})$ is the normalization factor.\n",
    "\n",
    "Note that if we know the natural parameter vector $\\phi(\\mathbf{\\theta})$, we can find the expectation of the natural statistic vector. Intuitively, this means that if we know the information about the parameters that the distribution uses to calculate the probability, that's all we know to find the expectation of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first the first summand of the last equation from the previous section:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{ Q(pa(j)) } \\left[ \\ln \\left( P( H_j | pa(j) ) \\right) \\right]\n",
    "$$\n",
    "\n",
    "This is only dependent on the parents of $j$, and therefore we need to ask what message node $j$ needs to receive from its parents to find this part of the summand, which in turn we need to find $\\textbf{Q}_j(\\textbf{H}_j)$.\n",
    "\n",
    "Since we assumed that the node's distribution is in the exponential family, we can rewrite this as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{ Q(pa(j)) } \\left[ \\ln \\left( e^{\\mathbf{\\theta} \\cdot \\mathbf{u} (\\mathbf{X}) + f(\\mathbf{X}) + g(\\mathbf{\\theta})} \\right) \\right] & = \\mathbb{E}_{ Q(pa(j)) } \\left[ \\mathbf{\\theta} \\cdot \\mathbf{u} (\\mathbf{X}) + f(\\mathbf{X}) + g(\\mathbf{\\theta}) \\right]\\\\\n",
    "& = \\mathbb{E}_{ Q(pa(j)) } \\left[ \\mathbf{\\theta} \\cdot \\mathbf{u} (\\mathbf{X}) \\right] + \\mathbb{E}_{ Q(pa(j)) } \\left[ f(\\mathbf{X}) \\right] + \\mathbb{E}_{ \n",
    "Q(pa(j)) } \\left[ g(\\mathbf{\\theta}) \\right] \\\\\n",
    "& = \\mathbb{E}_{ Q(pa(j)) } \\left[ \\mathbf{\\theta} \\right] \\cdot \\mathbf{u}(\\mathbf{X})  + f(\\mathbf{X}) + \\mathbb{E}_{ Q(pa(j)) } \\left[ g(\\mathbf{\\theta}) \\right] \n",
    "\\end{align}\n",
    "\n",
    "But the expected value of $\\mathbf{\\theta}$ is simply the expectations of the natural statistics of $j$'s parents. So that's the message that $j$ should receive from its parents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the derivation don't matter much here (you can find them online easily), but here's the conclusion:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{P(\\mathbf{X} \\mid \\mathbf{\\theta})} \\left[ \\mathbf{u} (\\mathbf{X}) \\right] = -\\frac{d g(\\mathbf{\\theta})}{d\\theta}\n",
    "$$\n",
    "\n",
    "So what does this mean? It means that we can find the expectation of the natural statistic of the parent of $j$ pretty easily, as the derivative of its normalization factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second bit we are interested in is:\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in ch(j)} \\mathbb{E}_{ Q(copa(j), ch(j)) } \\left[ \\ln \\left( P( i | pa(i)) \\right) \\right]\n",
    "$$\n",
    "\n",
    "Let's restrict our attention to $P( i | pa(i))$ for some specific child $i$ of $j$. Then $P( i | pa(i)$ equals $P( i | j, copa(j))$:\n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:computational_models]",
   "language": "python",
   "name": "conda-env-computational_models-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
